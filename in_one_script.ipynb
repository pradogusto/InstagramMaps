{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType\n",
    "import functions as fct\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.json('one_data_each_month/*.json')\n",
    "interest = df.select(\"_source.main\", \"_source.sentiment\", \"_source.lang\", \"_source.tags\", \"_source.date_found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11294"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interest.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_clean_udf = udf(lambda s: fct.first_clean(s), StringType())\n",
    "remove_features_udf = udf(lambda s: fct.remove_features(s), StringType())\n",
    "strip_accents_udf = udf(lambda s: fct.strip_accents(s), StringType())\n",
    "stops_udf = udf(lambda s: fct.remove_stops(s), StringType())\n",
    "\n",
    "# Remove instagram noise: # tags, @mentions and 'http://' url ..\n",
    "interest1 = interest.withColumn('main', first_clean_udf(interest.main))\n",
    "\n",
    "# Remove all accents \n",
    "interest2 = interest1.withColumn('main', strip_accents_udf(interest1.main))\n",
    "\n",
    "# Remove all punctuation except ., ! and ?, they could indicate a emotion sign \n",
    "# !!! for excitation or ... for annoyance\n",
    "interest3 = interest2.withColumn('main', remove_features_udf(interest2.main))\n",
    "\n",
    "# Remove english stopwords, all in lower case\n",
    "interest4 = interest3.withColumn('main', stops_udf(interest3.main))\n",
    "\n",
    "# Removing all text lower than 8 characters\n",
    "interest5 = interest4.filter(interest4.main!='')\n",
    "\n",
    "# saving result\n",
    "interest5.write.save(\"afterprocessing.parquet\",mode='overwrite', format=\"parquet\")\n",
    "after_process = sqlContext.read.parquet(\"afterprocessing.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with sentiment value [ML training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep in a dataframe those with sentiment given\n",
    "mlinterest = after_process.na.drop(subset=[\"sentiment\"])\n",
    "sent_value = udf(lambda s: fct.sentiment_values(s), IntegerType())\n",
    "\n",
    "# Change sentiment into numeric value 0, 1 and 2\n",
    "MLinterest = mlinterest.withColumn('label', sent_value(mlinterest.sentiment))\n",
    "\n",
    "# Keep only those in english\n",
    "MLINTEREST = MLinterest.filter(MLinterest.lang==\"en\")\n",
    "\n",
    "# Change of type for the ML pipeline\n",
    "MLINTEREST1 = MLINTEREST.withColumn(\"label\", MLINTEREST.label.cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a ML pipeline using TF-IDF for vectorization of words\n",
    "tokenizer = Tokenizer(inputCol=\"main\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "nb = NaiveBayes()\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, nb])\n",
    "\n",
    "# Train our model\n",
    "model = pipeline.fit(MLINTEREST1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#MLINTEREST1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z=MLINTEREST1.withColumnRenamed('label', 'true_label')\n",
    "pred =model.transform(z) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wesh=pred.select('prediction', 'true_label').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics = MulticlassMetrics(wesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats\n",
      "Precision = 0.9481446241674596\n",
      "Recall = 0.9481446241674596\n",
      "F1 Score = 0.9481446241674596\n"
     ]
    }
   ],
   "source": [
    "precision = metrics.precision()\n",
    "recall = metrics.recall()\n",
    "f1Score = metrics.fMeasure()\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# without sentiment value [ML Prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now keep in a dataframe those without sentiment value\n",
    "ml_topredict = after_process.filter(after_process.sentiment.isNull())\n",
    "\n",
    "# english detection using Langid library\n",
    "check_english_udf = udf(lambda s: fct.check_english(s), StringType())\n",
    "ml_topredict1 = ml_topredict.withColumn(\"lang\", check_english_udf(ml_topredict[\"main\"]))\n",
    "\n",
    "# keep only the english predicted instagrams\n",
    "ml_topredict2 = ml_topredict1.filter(ml_topredict1[\"lang\"] == \"en\")\n",
    "\n",
    "# Use our model (last section) to predict new sentiment values\n",
    "prediction = model.transform(ml_topredict2)\n",
    "prediction = prediction.withColumnRenamed('prediction', 'label').select('main','label','date_found','tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge the two dataframe to have all sentiments tpgether\n",
    "final0= MLINTEREST1.select('main','label','date_found', 'tags')\n",
    "final = prediction.unionAll(final0)\n",
    "mois_num_udf = udf(lambda s: fct.mois_num(s), StringType())\n",
    "final = final.withColumn('date_found', mois_num_udf(final['date_found']))\n",
    "\n",
    "# Save results\n",
    "final.write.save(\"final.parquet\",mode = 'overwrite', format=\"parquet\")\n",
    "load_final = sqlContext.read.parquet(\"final.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# localisation based on  hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  canton    id  len_name          name\n",
       " 0     GR  3501      10.0    alvaschein\n",
       " 1     SG  3403      12.0  ganterschwil\n",
       " 2     GR  3523       6.0        wiesen\n",
       " 3     GR  3522       7.0       filisur\n",
       " 4     GR  3521       6.0        bergun, (5750, 4))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a liste of all the municipalities in Switzerland and their canton\n",
    "final_df = fct.create_city_id()\n",
    "final_df.head(), final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "canton_udf=udf(lambda s: s.split(',')[1][:-1],StringType())\n",
    "city_udf=udf(lambda s: s.split(',')[0][1:],StringType())\n",
    "localise_udf = udf(lambda s: fct.localise(s, final_df), StringType())\n",
    "\n",
    "# Add a new column which give the city and the canton for each instagram, give -1 when nothing is found\n",
    "localize = load_final.withColumn('site', localise_udf(load_final.tags))\n",
    "localize1=localize.withColumn('canton',canton_udf(localize.site)).withColumn('city',city_udf(localize.site))\n",
    "\n",
    "# We remove all the instagrams where no location was find\n",
    "localize2 = localize1.filter(localize1.canton!='-1')\n",
    "\n",
    "# We save the results\n",
    "localize2.write.save(\"final_localization.parquet\",mode = 'overwrite', format=\"parquet\")\n",
    "load_final_local = sqlContext.read.parquet(\"final_localization.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization pre-processing\n",
    "\n",
    "Operations on dataframes for the final vizualization results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stats on the sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pie chart pos/neg/neutre \n",
    "pnn_pie = load_final.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pos/neg/neutre per month\n",
    "pnn_per_month = load_final.crosstab('label', 'date_found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tag study\n",
    "a = load_final.select('tags').flatMap(lambda line: line[0]).map(lambda tag: (tag,1)).reduceByKey(lambda x,y: x+y)\n",
    "tag_count = a.toDF()\n",
    "# stat about tags\n",
    "stats_tags = tag_count.describe()\n",
    "\n",
    "# Tag count for happy/neg/neutral instagrams\n",
    "for i in [0, 1, 2]:\n",
    "    # Tag count\n",
    "    a = load_final.filter(load_final.label==i).select('tags').flatMap(lambda line: line[0]).map(lambda tag: (tag,1)).reduceByKey(lambda x,y: x+y)\n",
    "    # Keep 1000 more used tags\n",
    "    sentiment_tags = text1.toDF().sort(F.desc(\"_2\")).limit(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stats on the geo-location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for canton map\n",
    "# MARKERS\n",
    "for i in [0,1,2]:\n",
    "    per_canton_month = load_final_local.filter(load_final_local['label']==i).crosstab('site', 'date_found')\n",
    "    #per_canton_month.write.save(\"per_canton_month.parquet\", format=\"parquet\")\n",
    "\n",
    "#CHOROPLETH MAP\n",
    "nb_hapinsta = load_final_local.filter(load_final_local[\"label\"]==0).groupBy('site').count()\n",
    "nb_insta = load_final_local.groupBy('site').count().withColumnRenamed('count', 'total')\n",
    "final_insta = nb_hapinsta.join(nb_insta, on = 'site')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For the city map\n",
    "city_map = load_final_local_city.groupBy('site').count()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
