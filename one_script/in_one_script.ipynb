{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType\n",
    "import functions as fct\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.json('data_example/*.json')\n",
    "interest = df.select(\"_source.main\", \"_source.sentiment\", \"_source.lang\", \"_source.tags\", \"_source.date_found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11294"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interest.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_clean_udf = udf(lambda s: fct.first_clean(s), StringType())\n",
    "remove_urls_udf = udf(lambda s: fct.remove_urls(s), StringType())\n",
    "strip_accents_udf = udf(lambda s: fct.strip_accents(s), StringType())\n",
    "filter_text_udf = udf(lambda s: fct.filter_text(s), StringType())\n",
    "\n",
    "# Remove instagram noise: # tags, @mentions and \n",
    "interest1 = interest.withColumn('main', first_clean_udf(interest.main))\n",
    "\n",
    "# Remove all accents \n",
    "interest2 = interest1.withColumn('main', strip_accents_udf(interest1.main))\n",
    "\n",
    "# Remove 'http://' url ..\n",
    "interest3 = interest2.withColumn('main', remove_urls_udf(interest2.main))\n",
    "\n",
    "# Remove english stopwords, and keep only words and emojis\n",
    "interest4 = interest3.withColumn('main', filter_text_udf(interest3.main))\n",
    "\n",
    "# Removing all text lower than 8 characters\n",
    "interest5 = interest4.filter(interest4.main!='')\n",
    "\n",
    "# saving result\n",
    "interest5.write.save(\"afterprocessing.parquet\",mode='overwrite', format=\"parquet\")\n",
    "after_process = sqlContext.read.parquet(\"afterprocessing.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with sentiment value [ML training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep in a dataframe those with sentiment given\n",
    "mlinterest = after_process.na.drop(subset=[\"sentiment\"])\n",
    "sent_value = udf(lambda s: fct.sentiment_values(s), IntegerType())\n",
    "\n",
    "# Change sentiment into numeric value 0, 1 and 2\n",
    "MLinterest = mlinterest.withColumn('label', sent_value(mlinterest.sentiment))\n",
    "\n",
    "# Keep only those in english\n",
    "MLINTEREST = MLinterest.filter(MLinterest.lang==\"en\")\n",
    "\n",
    "# Change of type for the ML pipeline\n",
    "MLINTEREST1 = MLINTEREST.withColumn(\"label\", MLINTEREST.label.cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a ML pipeline using TF-IDF for vectorization of words\n",
    "tokenizer = Tokenizer(inputCol=\"main\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "nb = NaiveBayes()\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, nb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train our model\n",
    "model = pipeline.fit(MLINTEREST1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = MLINTEREST1.randomSplit([0.6, 0.4], 24)\n",
    "train.cache()\n",
    "model = pipeline.fit(train)\n",
    "predictionAndLabels = model.transform(test.withColumnRenamed('label', 'true_label'))\n",
    "wesh=predictionAndLabels.select('prediction', 'true_label').rdd\n",
    "metrics = MulticlassMetrics(wesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in [0.,1.,2.]:\n",
    "    print('scores for', i, '\\n')\n",
    "    f1Score = metrics.fMeasure(i)\n",
    "    print(\"F1 Score = %s\" % f1Score, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(metrics.confusionMatrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# without sentiment value [ML Prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now keep in a dataframe those without sentiment value\n",
    "ml_topredict = after_process.filter(after_process.sentiment.isNull())\n",
    "\n",
    "# english detection using Langid library\n",
    "check_english_udf = udf(lambda s: fct.check_english(s), StringType())\n",
    "ml_topredict1 = ml_topredict.withColumn(\"lang\", check_english_udf(ml_topredict[\"main\"]))\n",
    "\n",
    "# keep only the english predicted instagrams\n",
    "ml_topredict2 = ml_topredict1.filter(ml_topredict1[\"lang\"] == \"en\")\n",
    "\n",
    "# Use our model (last section) to predict new sentiment values\n",
    "prediction = model.transform(ml_topredict2)\n",
    "prediction = prediction.withColumnRenamed('prediction', 'label').select('main','label','date_found','tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge the two dataframe to have all sentiments tpgether\n",
    "final0= MLINTEREST1.select('main','label','date_found', 'tags')\n",
    "final = prediction.unionAll(final0)\n",
    "mois_num_udf = udf(lambda s: fct.mois_num(s), StringType())\n",
    "final = final.withColumn('date_found', mois_num_udf(final['date_found']))\n",
    "\n",
    "# Save results\n",
    "final.write.save(\"final.parquet\",mode = 'overwrite', format=\"parquet\")\n",
    "load_final = sqlContext.read.parquet(\"final.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# localisation based on  hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  canton    id  len_name          name\n",
       " 0     GR  3501      10.0    alvaschein\n",
       " 1     SG  3403      12.0  ganterschwil\n",
       " 2     GR  3523       6.0        wiesen\n",
       " 3     GR  3522       7.0       filisur\n",
       " 4     GR  3521       6.0        bergun, (5750, 4))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a liste of all the municipalities in Switzerland and their canton\n",
    "final_df = fct.create_city_id()\n",
    "final_df.head(), final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "canton_udf=udf(lambda s: s.split(',')[1][:-1],StringType())\n",
    "city_udf=udf(lambda s: s.split(',')[0][1:],StringType())\n",
    "localise_udf = udf(lambda s: fct.localise(s, final_df), StringType())\n",
    "\n",
    "# Add a new column which give the city and the canton for each instagram, give -1 when nothing is found\n",
    "localize = load_final.withColumn('site', localise_udf(load_final.tags))\n",
    "localize1=localize.withColumn('canton',canton_udf(localize.site)).withColumn('city',city_udf(localize.site))\n",
    "\n",
    "# We remove all the instagrams where no location was find\n",
    "localize2 = localize1.filter(localize1.canton!='-1')\n",
    "\n",
    "# We save the results\n",
    "localize2.write.save(\"final_localization.parquet\",mode = 'overwrite', format=\"parquet\")\n",
    "load_final_local = sqlContext.read.parquet(\"final_localization.parquet\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
